# ğŸŒ©ï¸ Cloud-Native AI Inference Pipeline (SageMaker + Lambda + API Gateway)

This project demonstrates a **fully serverless, scalable, and production-ready AI inference system** built with AWS-native services. It features model training in **SageMaker**, deployment using **Lambda** behind an **API Gateway**, and infrastructure managed via **Terraform**. Ideal for real-time AI applications with minimal operational overhead.

---

## ğŸš€ Features

- ğŸ”§ **End-to-end pipeline**: From training to real-time inference
- ğŸ§  **Model training and deployment** on Amazon SageMaker
- âš™ï¸ **Inference via Lambda** using a RESTful API Gateway endpoint
- ğŸ—ï¸ **Infrastructure-as-Code (IaC)** using Terraform
- ğŸ” Optional **S3 integration**, **CloudWatch monitoring**, and environment-based control
- ğŸ“¦ Modular architecture suitable for continuous integration and rapid iteration

---

## ğŸ› ï¸ Tech Stack

| Tool              | Purpose                            |
|------------------|------------------------------------|
| AWS SageMaker     | Model training + hosting            |
| AWS Lambda        | Stateless inference handler         |
| Amazon API Gateway| Public API for inference requests   |
| Terraform         | Infrastructure provisioning         |
| Python            | ML model and logic                  |
| boto3             | AWS SDK for model invocation        |

---


---

# ğŸ“¦ MODEL FLOW (Train â†’ Deploy â†’ Infra â†’ Inference)

# 1ï¸âƒ£ Train Model
cd sagemaker/
pip install -r requirements.txt
python train_model.py

# 2ï¸âƒ£ Deploy SageMaker Model Endpoint
python deploy_model.py

# 3ï¸âƒ£ Provision Infrastructure via Terraform
cd ../terraform/
terraform init
terraform apply

# 4ï¸âƒ£ Trigger Inference via API Gateway
curl -X POST https://<api-gateway-url>/predict \
     -H "Content-Type: application/json" \
     -d '{"input": [1.5, 2.3, 3.1]}'


ğŸ“Š Monitoring & Logging
Logs are automatically pushed to Amazon CloudWatch

You can enhance observability with:

Alarms on Lambda errors

Tracing with AWS X-Ray

S3 logging for audit

ğŸ“Œ Use Cases
Real-time predictions for financial, healthcare, or e-commerce apps

Cost-effective ML inference without maintaining servers

Integrating ML into mobile or frontend apps via REST API

ğŸ§  Improvements (Future Work)
Add support for multiple models (via path params)

Add CI/CD with GitHub Actions to automate Terraform + Lambda deployment

Fine-tune API Gateway with throttling, caching, and authorization

Use EventBridge to retrain models on schedule

ğŸ‘¨â€ğŸ’» Author
M. Sulaiman Amir

ğŸŒ Website: sulaimanamir.com

ğŸ§  Credly: My Certificates

ğŸ’¼ LinkedIn: linkedin.com/in/m-sulaiman-amir-ab043632b

ğŸ“§ Email: sullemaan007@gmail.com

ğŸ§µ Discord: sulle_amir

âš ï¸ License
All Rights Reserved Â© 2025.
This code is for demonstration and educational purposes only.
You may not copy, modify, distribute, or use it in production without permission.

---


